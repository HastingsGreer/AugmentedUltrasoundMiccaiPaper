\relax 
\@writefile{toc}{\contentsline {chapter}{Augmented Ultrasound}{0}}
\@writefile{toc}{\contentsline {title}{Ultrasound Augmentation: Rapid 3-D Scanning for Tracking and On-Body Display}{1}}
\@writefile{toc}{\authcount {7}}
\@writefile{toc}{\contentsline {author}{Maeliss Jallais \and Hastings Greer \and Sam Gerber \and Matt McCormick \and Deepak Chittajallu \and Neal Siekierski \and Stephen Aylward}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Motivation}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Augmented Ultrasound}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}3D Scene Modeling}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  The augmented reality system uses a pico projector (based on laser projection technology) to display an image in a scene. A high-speed camera sees the individual lines being drawn by the projector to create the image. Via appropriate camera calibration, custom circuitry to detect the vertical reset of the laser and depth from structured lighting algorithms, a 3D model of the scene can be formed at very high resolution in about one second. \relax }}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hardware used, with prices\relax }}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Object Tracking}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  (a) Circuit diagram of the photo detector system used to determine when the laser returns to the upper left and begins drawing a new image (approximately every 1/60th of a second). (b) Oscilliscope-measured photocell output (yellow) and circuit output (blue) signals. The circuit output's falling edge is used as a camera trigger. \relax }}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Examples of 3D scenes captured by our system. \relax }}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  (a) The setup of the system, with the projector augmenting the scene by adding lines and a yellow instruction card. (b) The collected point cloud is shown in white. The intersection of the estimated red, green and blue planes with that point cloud are shown. This indicates that the faces of the cube are well represented by the estimated planes. (c) A close-up of the scene shows the projected lines and the projected yellow instruction card (labled "test image") that automatically follows the tip of the ultrasound probe. \relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The behavior of error as the number of lines per reconstruction is increased at a distance of 60 cm\relax }}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Representative error in x, y, and z at a distance of 60 cm with 45 lines per reconstruction\relax }}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracy at 60cm and 400Lux on 300 acquisitions\relax }}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Projecting onto the Scene}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{7}}
\bibcite{1}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An illustration of the end goal of the system. A target (black X) is projected onto the skin as the camera computes depth from structured light using the raster lines of the projected image as that image is drawn. Ultrasound can be used to detect peripheral vessels, select needle insertion locations and verify needle placement patency. \relax }}{8}}
\bibcite{2}{2}
\bibcite{3}{3}
\bibcite{4}{4}
