\relax 
\@writefile{toc}{\contentsline {chapter}{Augmented Ultrasound}{0}}
\@writefile{toc}{\contentsline {title}{Ultrasound Augmentation: Rapid 3-D Scanning for Tracking and On Body Display}{1}}
\@writefile{toc}{\authcount {3}}
\@writefile{toc}{\contentsline {author}{Maeliss Jallais \and Hastings Greer \and Stephen Aylward}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Motivation}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Augmented Ultrasound}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}3D Scene Modeling}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  The augmented reality system uses a pico projector (based on laser projection technology) to display an image in a scene. A high-speed camera sees the individual lines being drawn by the projector to create the image. Via appropriate camera calibration, custom circuitry to detect the vertical reset of the laser and depth from structured lighting algorithms, a 3D model of the scene can be formed at very high resolution in about one second. \relax }}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hardware used, with prices\relax }}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Circuit diagram of the photo detector system used to determine when the laser returns to the upper left and begins drawing a new image (approximately every 1/60th of a second). \relax }}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Examples of 3D scenes captured by our system. \relax }}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Object Tracking}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  (a) The raw point cloud before cube detection, (b) The collected point cloud is shown in white. The intersection of the estimated red, green and blue planes with that point cloud are shown. This indicates that the faces of the cube are well represented by the estimated planes. \relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The behavior of error as the number of lines per reconstruction is increased at a distance of 60 cm\relax }}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Projecting onto the Scene}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Representative error in x, y, and z at a distance of 60 cm with 45 lines per reconstruction\relax }}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracy at 60cm and 400Lux on 300 acquisitions\relax }}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}}
\bibcite{1}{1}
\bibcite{2}{2}
\bibcite{3}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An illustration of the end goal of the system. A target (black X) is projected onto the skin as the camera computes depth from structured light using the raster lines of the projected image as that image is drawn. Ultrasound can be used to detect peripheral vessels, select needle insertion locations and verify needle placement patency. \relax }}{9}}
